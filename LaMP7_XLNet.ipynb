{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6e6501c5-81b2-4a90-85bb-ed72af91ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer, XLNetModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import json\n",
    "\n",
    "file_path = 'tweet.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    dataset = json.load(file)\n",
    "    \n",
    "# Load pre-trained XLNet tokenizer and model\n",
    "# tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "# model = XLNetModel.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b257f00c-9dc3-40a9-931c-fc03c84a2fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9e1132d1-aa94-409d-b890-7139e3a54f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>input</th>\n",
       "      <th>profile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>600</td>\n",
       "      <td>Paraphrase the following tweet without any exp...</td>\n",
       "      <td>[{'text': 'SARS .. H1N1 .. Air France ..  plea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>601</td>\n",
       "      <td>Paraphrase the following tweet without any exp...</td>\n",
       "      <td>[{'text': '@kAtrinaDaniels never thought that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>602</td>\n",
       "      <td>Paraphrase the following tweet without any exp...</td>\n",
       "      <td>[{'text': '@mattmaloney I feel cheered up. Wow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>603</td>\n",
       "      <td>Paraphrase the following tweet without any exp...</td>\n",
       "      <td>[{'text': 'Night all. Watched Mamma Mia. Did I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>604</td>\n",
       "      <td>Paraphrase the following tweet without any exp...</td>\n",
       "      <td>[{'text': '@hoffifer working, as usual .. Awes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              input  \\\n",
       "0  600  Paraphrase the following tweet without any exp...   \n",
       "1  601  Paraphrase the following tweet without any exp...   \n",
       "2  602  Paraphrase the following tweet without any exp...   \n",
       "3  603  Paraphrase the following tweet without any exp...   \n",
       "4  604  Paraphrase the following tweet without any exp...   \n",
       "\n",
       "                                             profile  \n",
       "0  [{'text': 'SARS .. H1N1 .. Air France ..  plea...  \n",
       "1  [{'text': '@kAtrinaDaniels never thought that ...  \n",
       "2  [{'text': '@mattmaloney I feel cheered up. Wow...  \n",
       "3  [{'text': 'Night all. Watched Mamma Mia. Did I...  \n",
       "4  [{'text': '@hoffifer working, as usual .. Awes...  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame.from_dict(dataset)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "76541a41-cf79-43f8-945c-f8314abe6228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@MINGOENT, that's great news! My children are doing well. School is almost finished, but I'm not quite ready for it. LOL\n"
     ]
    }
   ],
   "source": [
    "query_start = 'Paraphrase the following tweet without any explanation before or after it: '\n",
    "query = dataset[6]['input'][len(query_start):]\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f890793d-0348-4c2e-a28d-56ff5aa4f7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for doc in dataset[6]['profile']:\n",
    "    documents.append(doc['text'])\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "801f95be-82ae-4e4e-a792-0e3f5beddca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "dd8baed9-19ac-4821-a4dd-a47de2fb5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "02479960-083a-4e9f-a73b-97d2ab9ef044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained XLNet tokenizer and model\n",
    "tokenizer_xlnet = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "model_xlnet = XLNetModel.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8ceac813-bb43-4d2f-b443-2265d5fa1a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "user_profile = dataset[0][\"profile\"]\n",
    "print(len(user_profile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6f7f833a-0d15-4e6d-8eec-01604f2b3fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embedding = []\n",
    "# Process each document in the first user profile\n",
    "for profile in user_profile:\n",
    "    # Tokenize and extract embeddings for each document\n",
    "    tokenized_document = tokenizer_xlnet(profile[\"text\"], return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        document_outputs = model_xlnet(**tokenized_document)\n",
    "    # Use the last layer output as the document embedding\n",
    "    document_embedding.append(document_outputs.last_hidden_state.mean(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "81cdda0c-ba22-4889-bd17-e1dbcd15b2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store relevant documents and scores for each version\n",
    "all_relevant_documents = []\n",
    "all_relevant_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e9c2e23a-7a7b-45e7-9f67-ba0212df4114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Input Version 1: Visit the blog for the best quote of the day, which is \"cake my face, BITCH\" hahaha.\n",
      "==================================================\n",
      "Original Score: tensor([0.9781])\n",
      "Document: Ok these eggs and bacon were 2 gooooood. Turkey bacon that is   these kids better come eat b4 I eat there food. *snackinOnkidsfoods* lol\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9777])\n",
      "Document: @KiKiDeuce yeah legal probs/never fun! And I only get a week off school  its a 2 yr program. Booo buts all good\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9769])\n",
      "Document: I just installed more memory and a new modem in my computer. And I believe my nails are gonna have 2 go.  dame!\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9761])\n",
      "Document: @maritorres Im missed my final Mama, I was locked up.  Hopefully my grade was high anough for me to pass the class\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9750])\n",
      "Document: @TearlessPoet I forgot my phone , so no twitting  but anyways how was your wkd?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Processing Input Version 2: Head over to the blog for the most impressive quote of the day, which is \"cake my face, BITCH\" hahaha.\n",
      "==================================================\n",
      "Original Score: tensor([0.9742])\n",
      "Document: Ok these eggs and bacon were 2 gooooood. Turkey bacon that is   these kids better come eat b4 I eat there food. *snackinOnkidsfoods* lol\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9722])\n",
      "Document: @maritorres Im missed my final Mama, I was locked up.  Hopefully my grade was high anough for me to pass the class\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9718])\n",
      "Document: @KiKiDeuce yeah legal probs/never fun! And I only get a week off school  its a 2 yr program. Booo buts all good\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9714])\n",
      "Document: @TearlessPoet I forgot my phone , so no twitting  but anyways how was your wkd?\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9711])\n",
      "Document: I just installed more memory and a new modem in my computer. And I believe my nails are gonna have 2 go.  dame!\n",
      "\n",
      "==================================================\n",
      "\n",
      "Processing Input Version 3: Check out the blog today where you can find the most impressive quote of the day, which is \"cake my face, BITCH\" hahaha.\n",
      "==================================================\n",
      "Original Score: tensor([0.9731])\n",
      "Document: @KiKiDeuce yeah legal probs/never fun! And I only get a week off school  its a 2 yr program. Booo buts all good\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9727])\n",
      "Document: Ok these eggs and bacon were 2 gooooood. Turkey bacon that is   these kids better come eat b4 I eat there food. *snackinOnkidsfoods* lol\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9723])\n",
      "Document: @maritorres Im missed my final Mama, I was locked up.  Hopefully my grade was high anough for me to pass the class\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9714])\n",
      "Document: I just installed more memory and a new modem in my computer. And I believe my nails are gonna have 2 go.  dame!\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9714])\n",
      "Document: @TearlessPoet I forgot my phone , so no twitting  but anyways how was your wkd?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Processing Input Version 4: Visit the blog for the most impressive quote of today, which is \"cake my face, BITCH\" hahaha.\n",
      "==================================================\n",
      "Original Score: tensor([0.9766])\n",
      "Document: @KiKiDeuce yeah legal probs/never fun! And I only get a week off school  its a 2 yr program. Booo buts all good\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9756])\n",
      "Document: Ok these eggs and bacon were 2 gooooood. Turkey bacon that is   these kids better come eat b4 I eat there food. *snackinOnkidsfoods* lol\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9756])\n",
      "Document: I just installed more memory and a new modem in my computer. And I believe my nails are gonna have 2 go.  dame!\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9752])\n",
      "Document: @maritorres Im missed my final Mama, I was locked up.  Hopefully my grade was high anough for me to pass the class\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9747])\n",
      "Document: @TearlessPoet I forgot my phone , so no twitting  but anyways how was your wkd?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Processing Input Version 5: The most popular quote on the blog is \"BITCH, cake my face\"... Read on for the best quotes of the day.\n",
      "==================================================\n",
      "Original Score: tensor([0.9793])\n",
      "Document: Ok these eggs and bacon were 2 gooooood. Turkey bacon that is   these kids better come eat b4 I eat there food. *snackinOnkidsfoods* lol\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9783])\n",
      "Document: @KiKiDeuce yeah legal probs/never fun! And I only get a week off school  its a 2 yr program. Booo buts all good\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9773])\n",
      "Document: @TearlessPoet lol that would be ok 2! I'm always jumpin in the pool lol....I'm telling u, You can't ruin my day. \n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9771])\n",
      "Document: Thanks for the follow new tweeps...but if you have no pic or your a company 9/10 I wont follow you  sorry...I hate all the ads\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9771])\n",
      "Document: @maritorres Im missed my final Mama, I was locked up.  Hopefully my grade was high anough for me to pass the class\n",
      "\n",
      "==================================================\n",
      "\n",
      "Processing Input Version 6: To see the top quote of today, head over to your blog and say \"cake my face, BITCH\" incessantly!\n",
      "==================================================\n",
      "Original Score: tensor([0.9753])\n",
      "Document: @maritorres Im missed my final Mama, I was locked up.  Hopefully my grade was high anough for me to pass the class\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9724])\n",
      "Document: @TearlessPoet lol that would be ok 2! I'm always jumpin in the pool lol....I'm telling u, You can't ruin my day. \n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9706])\n",
      "Document: @KiKiDeuce yeah legal probs/never fun! And I only get a week off school  its a 2 yr program. Booo buts all good\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9700])\n",
      "Document: Thanks for the follow new tweeps...but if you have no pic or your a company 9/10 I wont follow you  sorry...I hate all the ads\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9691])\n",
      "Document: @TearlessPoet I forgot my phone , so no twitting  but anyways how was your wkd?\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "j=5\n",
    "for i, input_version in enumerate([input_sentence] + rephrased_versions, start=1):\n",
    "    # Print the input version\n",
    "    print(f\"\\nProcessing Input Version {i}: {input_version}\\n{'='*50}\")\n",
    "\n",
    "    # Retrieve relevant documents for the current version\n",
    "    input_documents = []\n",
    "    input_scores = []\n",
    "\n",
    "    # Encode the current query version\n",
    "    tokenized_paraphrase_rob = tokenizer_xlnet(f'paraphrase: {input_version}', return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "            paraphrase_outputs = model_xlnet(**tokenized_paraphrase_rob)\n",
    "    paraphrase_embedding = paraphrase_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    # Calculate cosine similarity between the query and paraphrase embeddings\n",
    "    # similarity_scores = util.dot_score(paraphrase_embedding, document_embedding)[0].cpu().tolist()\n",
    "    similarity = []\n",
    "    for j in range(len(document_embedding)):\n",
    "        similarity.append(torch.nn.functional.cosine_similarity(document_embedding[j], paraphrase_embedding.squeeze(dim=1)))\n",
    "    doc_score_title_pairs = list(zip([doc['text'] for doc in dataset[6]['profile']],\n",
    "                                     similarity))\n",
    "    \n",
    "    # Sort by decreasing similarity score\n",
    "    doc_score_title_pairs = sorted(doc_score_title_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Calculate the number of documents to retrieve (top 25%)\n",
    "    num_documents_to_retrieve = int(0.25 * len(doc_score_title_pairs))\n",
    "\n",
    "    # Check if the number of documents to retrieve is greater than 15\n",
    "    if num_documents_to_retrieve > 10:\n",
    "        num_documents_to_retrieve = 10\n",
    "\n",
    "    if num_documents_to_retrieve < 5:\n",
    "        num_documents_to_retrieve = 5\n",
    "        \n",
    "    for text, score in doc_score_title_pairs[:num_documents_to_retrieve]:\n",
    "        print(f\"Original Score: {score}\")\n",
    "        print(f\"Document: {text}\\n\")\n",
    "        \n",
    "        # Store relevant document, title, and score for each interpretation\n",
    "        input_documents.append({'text': text})\n",
    "        input_scores.append(score)\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        \n",
    "    # Store relevant documents and scores for each version\n",
    "    all_relevant_documents.append(input_documents)\n",
    "    all_relevant_scores.append(input_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "5c324993-4d0e-49c8-acd4-fbfbdd169f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top  Documents :\n",
      "[{'text': \"sad  didn't win any prize for PC Show lucky draw .. The 12th and 13th prize are consecutive numbers .. Must be the same person &gt;_&lt;\"}, {'text': \"I have exceeded my Twitter API limit. Gosh. Was too excited about Singapore trending .. Can't tweet anymore  anyway i am going for a jog\"}, {'text': 'Ok these eggs and bacon were 2 gooooood. Turkey bacon that is   these kids better come eat b4 I eat there food. *snackinOnkidsfoods* lol'}, {'text': \"good night all. I am sleeping early tonight. If not I will free so tired like today .. And i can't go for a jog  sleep tight !!\"}, {'text': \"@waxyx hmmm if it's not at 3pm (12am California time) we might have to wait till 1am  .. That's 10am California time ..\"}, {'text': \"addicted to twitter. Time to get out of bed. It's monday \"}, {'text': \"@TutyFruitty Oh no .. Singapore isn't trending anymore .. that's sad \"}, {'text': \"@TearlessPoet lol that would be ok 2! I'm always jumpin in the pool lol....I'm telling u, You can't ruin my day. \"}, {'text': '@maritorres Im missed my final Mama, I was locked up.  Hopefully my grade was high anough for me to pass the class'}, {'text': \"it's friday !! And i just got on the bus .. Going to work later today again \"}]\n"
     ]
    }
   ],
   "source": [
    "# Calculate average score for each document across different interpretations\n",
    "average_scores = {}\n",
    "for documents, scores in zip(all_relevant_documents, all_relevant_scores):\n",
    "    for doc_dict, score in zip(documents, scores):\n",
    "        if isinstance(doc_dict, dict):  # Check if it's a dictionary\n",
    "            doc_text = doc_dict.get('text', '')  # Use 'get' to provide a default value if 'text' is not present\n",
    "            if doc_text:\n",
    "                if doc_text not in average_scores:\n",
    "                    average_scores[doc_text] = {'text': doc_text, 'scores': []}\n",
    "                average_scores[doc_text]['scores'].append(score)\n",
    "\n",
    "# Calculate average score for each document\n",
    "average_documents = [{'text': details['text'], 'average_score': sum(details['scores']) / len(details['scores'])}for details in average_scores.values()]\n",
    "\n",
    "# Sort documents based on average scores\n",
    "sorted_documents = sorted(average_documents, key=lambda x: x['average_score'], reverse=True)\n",
    "\n",
    "retrieval_docs = []\n",
    "\n",
    "# Output top 10 documents based on average scores\n",
    "print(\"\\nTop  Documents :\")\n",
    "for doc_dict in sorted_documents[:10]:\n",
    "    new_doc_dict = {'text': doc_dict['text']}\n",
    "    retrieval_docs.append(new_doc_dict)\n",
    "print(retrieval_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ffc760f3-ba62-4f8b-9621-94f1b2c4e7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The text is: sad  didn't win any prize for PC Show lucky draw .. The 12th and 13th prize are consecutive numbers .. Must be the same person &gt;_&lt;\n",
      "2. The text is: I have exceeded my Twitter API limit. Gosh. Was too excited about Singapore trending .. Can't tweet anymore  anyway i am going for a jog\n",
      "3. The text is: Ok these eggs and bacon were 2 gooooood. Turkey bacon that is   these kids better come eat b4 I eat there food. *snackinOnkidsfoods* lol\n",
      "4. The text is: good night all. I am sleeping early tonight. If not I will free so tired like today .. And i can't go for a jog  sleep tight !!\n",
      "5. The text is: @waxyx hmmm if it's not at 3pm (12am California time) we might have to wait till 1am  .. That's 10am California time ..\n",
      "6. The text is: addicted to twitter. Time to get out of bed. It's monday \n",
      "7. The text is: @TutyFruitty Oh no .. Singapore isn't trending anymore .. that's sad \n",
      "8. The text is: @TearlessPoet lol that would be ok 2! I'm always jumpin in the pool lol....I'm telling u, You can't ruin my day. \n",
      "9. The text is: @maritorres Im missed my final Mama, I was locked up.  Hopefully my grade was high anough for me to pass the class\n",
      "10. The text is: it's friday !! And i just got on the bus .. Going to work later today again \n",
      "\n"
     ]
    }
   ],
   "source": [
    "r_d = ''\n",
    "count = 1\n",
    "for i in retrieval_docs:\n",
    "  # print(i)\n",
    "  r_d += str(count) + '. '\n",
    "  r_d += 'The text is: ' + i['text'] + '\\n'\n",
    "  count += 1\n",
    "  # if count > 7:\n",
    "  #   break\n",
    "\n",
    "print(r_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7e745b5d-1172-4af6-9c2a-3ea85f207a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(retrieval_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "232a1e50-2ed5-44cc-8062-b9b509ddaf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U -q google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "bdc14a7b-f693-449c-b9ea-bb713b28c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as palm\n",
    "\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8ba2449d-4320-4209-96a6-dd91b9ea2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "palm.configure(api_key='AIzaSyBVo_JbfzrPBpHbueQtOiRozzyFK1QK8D0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "159be4f3-9869-4220-bd83-4d8b1d5b135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(query, relevant_passage):\n",
    "  escaped = relevant_passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
    "  prompt = textwrap.dedent(\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below.   \n",
    "  Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. I'm providing you with some sample tweets written by me for 9 texts examples.\n",
    "  Based on the context provided, can you paraphrase the query to capture my writing style. Make sure it looks like its written by me.\n",
    "  QUESTION: '{query}'\n",
    "  PASSAGE: '{relevant_passage}'\n",
    "\n",
    "    ANSWER:\n",
    "  \"\"\").format(query=query, relevant_passage=escaped)\n",
    "\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e43da860-479e-4538-8643-89778d8bfe03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful and informative bot that answers questions using text from the reference passage included below.   \n",
      "  Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. I'm providing you with some sample tweets written by me for 9 texts examples.\n",
      "  Based on the context provided, can you paraphrase the query to capture my writing style. Make sure it looks like its written by me.\n",
      "  QUESTION: '@MINGOENT, that's great news! My children are doing well. School is almost finished, but I'm not quite ready for it. LOL'\n",
      "  PASSAGE: '1. The text is: @waxyx I dont know .. I wanted to restart it .. I switch it off and it wont turn on again  2. The text is: addicted to twitter. Time to get out of bed. Its monday  3. The text is: its friday !! And i just got on the bus .. Going to work later today again  4. The text is: SARS .. H1N1 .. Air France ..  please cherish your life, people .. 5. The text is: i am at interchange .. Just missed the bus  6. The text is: @waxyx informatics , do u know that? (via @waxyx)no I meant which school haha  I am in ntu 7. The text is: listening to easons 2006 album .. Whats going on...? This is my favourite eason album  its 3.38am 8. The text is: &quot;See ... You make the world go weird ...&quot; from weiweis SMS  9. The text is: Finished blogging .. continue to rate restaurants on Facebook .. I wanna get the trophy after rating 100 restaurants  '\n",
      "\n",
      "    ANSWER:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = make_prompt(query, passage)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "2e696d3d-8c14-47d4-8deb-669bb35d568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "\n",
    "text_model = text_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "f28ed187-f187-48ae-ae85-4b506fe6e6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.6\n",
    "answer = palm.generate_text(prompt=prompt,\n",
    "                            model=text_model,\n",
    "                            candidate_count=3,\n",
    "                            temperature=temperature,\n",
    "                            max_output_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "91baeebd-ab68-44ac-b586-25eb3f23253c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 1: @MINGOENT, good to hear that! School is almost over but I'm not quite ready yet. LOL\n",
      "\n",
      "Candidate 2: @MINGOENT, that's great news! My children are doing well. School is almost finished, but I'm not quite ready for it. LOL\n",
      "\n",
      "Candidate 3: @MINGOENT, that's great news! My children are doing well too. School is almost finished, but I'm not quite ready for it. LOL.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, candidate in enumerate(answer.candidates):\n",
    "  print(f\"Candidate {i+1}: {candidate['output']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d082b53-e600-4d08-805d-b338ec499ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
