{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e30dc4e4-463e-4060-87f2-43b0c8a0d6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (4.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: filelock in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: requests in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fad4a10f-a6f9-4500-be59-238a479159bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (2.1.1)\n",
      "Requirement already satisfied: sympy in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: typing-extensions in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: networkx in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: filelock in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: fsspec in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0584cd62-c481-4bd3-9463-d14bc57b717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "78168d9e-f3ad-48c4-8e8a-3055fb0e9919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb72275f-4dbb-4dd6-91ea-8f8458d76d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from sentence_transformers) (4.36.0)\n",
      "Requirement already satisfied: tqdm in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from sentence_transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: torchvision in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from sentence_transformers) (0.16.1)\n",
      "Requirement already satisfied: numpy in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from sentence_transformers) (1.26.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from sentence_transformers) (1.11.4)\n",
      "Requirement already satisfied: nltk in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from sentence_transformers) (0.19.4)\n",
      "Requirement already satisfied: filelock in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: requests in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: jinja2 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: networkx in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n",
      "Requirement already satisfied: click in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from nltk->sentence_transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from nltk->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torchvision->sentence_transformers) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d00114a-104e-4f47-9e23-c737d308a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3785e86b-f3c0-40b2-9965-5d7e1b9bdab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (2.1.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "062e0803-7347-40ba-8543-42146690066f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>input</th>\n",
       "      <th>profile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>600</td>\n",
       "      <td>Paraphrase the following tweet without any exp...</td>\n",
       "      <td>[{'text': 'SARS .. H1N1 .. Air France ..  plea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>601</td>\n",
       "      <td>Paraphrase the following tweet without any exp...</td>\n",
       "      <td>[{'text': '@kAtrinaDaniels never thought that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>602</td>\n",
       "      <td>Paraphrase the following tweet without any exp...</td>\n",
       "      <td>[{'text': '@mattmaloney I feel cheered up. Wow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>603</td>\n",
       "      <td>Paraphrase the following tweet without any exp...</td>\n",
       "      <td>[{'text': 'Night all. Watched Mamma Mia. Did I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>604</td>\n",
       "      <td>Paraphrase the following tweet without any exp...</td>\n",
       "      <td>[{'text': '@hoffifer working, as usual .. Awes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              input  \\\n",
       "0  600  Paraphrase the following tweet without any exp...   \n",
       "1  601  Paraphrase the following tweet without any exp...   \n",
       "2  602  Paraphrase the following tweet without any exp...   \n",
       "3  603  Paraphrase the following tweet without any exp...   \n",
       "4  604  Paraphrase the following tweet without any exp...   \n",
       "\n",
       "                                             profile  \n",
       "0  [{'text': 'SARS .. H1N1 .. Air France ..  plea...  \n",
       "1  [{'text': '@kAtrinaDaniels never thought that ...  \n",
       "2  [{'text': '@mattmaloney I feel cheered up. Wow...  \n",
       "3  [{'text': 'Night all. Watched Mamma Mia. Did I...  \n",
       "4  [{'text': '@hoffifer working, as usual .. Awes...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your JSON file in Google Drive\n",
    "file_path = \"tweet.json\"\n",
    "\n",
    "# Load data from the file\n",
    "with open(file_path, \"r\") as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "data = pd.DataFrame.from_dict(dataset)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9e294e05-f17e-4560-a60e-d905826e7bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrase the following tweet without any explanation before or after it: @MissPressa and I had the same idea. Unfortunately, she did not attend SMCSYD. I noticed her experience via Twitter.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[5]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cd557c01-1e85-4f9a-af66-1929600bf142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@MissPressa and I had the same idea. Unfortunately, she did not attend SMCSYD. I noticed her experience via Twitter.\n",
      "605\n"
     ]
    }
   ],
   "source": [
    "query_start = 'Paraphrase the following tweet without any explanation before or after it: '\n",
    "query = dataset[5]['input'][len(query_start):]\n",
    "print(query)\n",
    "print(dataset[5]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "79a52e00-206e-45e9-8bba-3a139d1f6506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "documents=[]\n",
    "for doc in dataset[5]['profile']:\n",
    "    documents.append(doc['text']) \n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "088da4f4-6cc2-4ede-adf6-c00086f3ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rephrased Version 1: I shared with @MissPressa that we both had a similar idea, but she wasn't accepted into SMCSYD. I followed her on Twitter.\n",
      "-------------------------------------------\n",
      "Rephrased Version 2: I had the same thought as @MissPressa, but she wasn't in SMCSYD. I followed her on Twitter now.\n",
      "-------------------------------------------\n",
      "Rephrased Version 3: @MissPressa and I had the same idea but she did not attend SMCSYD. I followed her on Twitter after reading about it.\n",
      "-------------------------------------------\n",
      "Rephrased Version 4: My thoughts and @MissPressa were in agreement. Regrettably, she did not attend SMCSYD. I caught sight of her story on Twitter.\n",
      "-------------------------------------------\n",
      "Rephrased Version 5: The thought occurred to me by using @MissPressa, but she did not attend SMCSYD. I caught sight of her story on Twitter.\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def paraphrase_t5(\n",
    "    input_sentence,\n",
    "    num_beams=5,\n",
    "    num_beam_groups=5,\n",
    "    num_return_sequences=5,\n",
    "    repetition_penalty=10.0,\n",
    "    diversity_penalty=3.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=0.7,\n",
    "    max_length=500\n",
    "):\n",
    "    input_ids = tokenizer(\n",
    "        f'paraphrase: {input_sentence}',\n",
    "        return_tensors=\"pt\", padding=\"longest\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n",
    "        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_beams=num_beams, num_beam_groups=num_beam_groups,\n",
    "        max_length=max_length, diversity_penalty=diversity_penalty\n",
    "    )\n",
    "\n",
    "    rephrased_versions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    return rephrased_versions\n",
    "\n",
    "# Example usage\n",
    "input_sentence = query\n",
    "rephrased_versions = paraphrase_t5(input_sentence)\n",
    "\n",
    "\n",
    "for i, rephrased_sentence in enumerate(rephrased_versions, start=1):\n",
    "    print(f\"Rephrased Version {i}: {rephrased_sentence}\")\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "13b1bc6d-d006-46e4-a025-e3ee65fce510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "model2 = SentenceTransformer('sentence-transformers/msmarco-distilbert-cos-v5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "da540769-66d9-416f-a415-2500a2a4d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode query and documents\n",
    "# query_emb = model2.encode(query)\n",
    "doc_emb = model2.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eb6b77e2-4474-44d2-8a7e-6bf0759239cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store relevant documents and scores for each version\n",
    "all_relevant_documents = []\n",
    "all_relevant_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "98e0e303-42b7-488a-9074-6217b14bee73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Input Version 1: @MissPressa and I had the same idea. Unfortunately, she did not attend SMCSYD. I noticed her experience via Twitter.\n",
      "==================================================\n",
      "Original Score: 0.3484240472316742\n",
      "Document: @adam_butler  Hey. It's a date! You should def come along. It's a good night. Follow @SMCSYD for updates (if you're not already). \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.3471030592918396\n",
      "Document: @cazduck Me too. You can refresh your memory of it here if you want to:   \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.2714482247829437\n",
      "Document: @cazduck Oh - City to Surf is on 9 Aug... so won't be here... \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.2549847662448883\n",
      "Document: @cazduck Sorry. It's very odd, I know. Did you get anything? Still feeling very blah &amp; in pain when I move my head or arm too. Not ideal. \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.2530112862586975\n",
      "Document: @thejenbug Good work jumping on board with #80sday jenbug. \n",
      "\n",
      "==================================================\n",
      "\n",
      "Processing Input Version 2: I shared with @MissPressa that we both had a similar idea, but she wasn't accepted into SMCSYD. I followed her on Twitter.\n",
      "==================================================\n",
      "Original Score: 0.3474753201007843\n",
      "Document: @adam_butler  Hey. It's a date! You should def come along. It's a good night. Follow @SMCSYD for updates (if you're not already). \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.2813863456249237\n",
      "Document: @cazduck Me too. You can refresh your memory of it here if you want to:   \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.24891704320907593\n",
      "Document: @cazduck Oh - City to Surf is on 9 Aug... so won't be here... \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.24283653497695923\n",
      "Document: @cazduck Sorry. It's very odd, I know. Did you get anything? Still feeling very blah &amp; in pain when I move my head or arm too. Not ideal. \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.23328924179077148\n",
      "Document: @thejenbug Good work jumping on board with #80sday jenbug. \n",
      "\n",
      "==================================================\n",
      "\n",
      "Processing Input Version 3: I had the same thought as @MissPressa, but she wasn't in SMCSYD. I followed her on Twitter now.\n",
      "==================================================\n",
      "Original Score: 0.33644235134124756\n",
      "Document: @cazduck Me too. You can refresh your memory of it here if you want to:   \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.310824990272522\n",
      "Document: @adam_butler  Hey. It's a date! You should def come along. It's a good night. Follow @SMCSYD for updates (if you're not already). \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.294489324092865\n",
      "Document: @cazduck Sorry. It's very odd, I know. Did you get anything? Still feeling very blah &amp; in pain when I move my head or arm too. Not ideal. \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.27300798892974854\n",
      "Document: @cazduck Oh - City to Surf is on 9 Aug... so won't be here... \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.23874521255493164\n",
      "Document: @thejenbug Good work jumping on board with #80sday jenbug. \n",
      "\n",
      "==================================================\n",
      "\n",
      "Processing Input Version 4: @MissPressa and I had the same idea but she did not attend SMCSYD. I followed her on Twitter after reading about it.\n",
      "==================================================\n",
      "Original Score: 0.3737090826034546\n",
      "Document: @adam_butler  Hey. It's a date! You should def come along. It's a good night. Follow @SMCSYD for updates (if you're not already). \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.3236793875694275\n",
      "Document: @cazduck Me too. You can refresh your memory of it here if you want to:   \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.26300424337387085\n",
      "Document: @cazduck Oh - City to Surf is on 9 Aug... so won't be here... \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.24829117953777313\n",
      "Document: @tiphereth Ta. Must be new profile pic day.  Thailand top of holiday list so far. Not for a few months though - probably a winter escape.\n",
      "\n",
      "==================================================\n",
      "Original Score: 0.2474879026412964\n",
      "Document: @cazduck Sorry. It's very odd, I know. Did you get anything? Still feeling very blah &amp; in pain when I move my head or arm too. Not ideal. \n",
      "\n",
      "==================================================\n",
      "\n",
      "Processing Input Version 5: My thoughts and @MissPressa were in agreement. Regrettably, she did not attend SMCSYD. I caught sight of her story on Twitter.\n",
      "==================================================\n",
      "Original Score: 0.3443288207054138\n",
      "Document: @cazduck Me too. You can refresh your memory of it here if you want to:   \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.34079742431640625\n",
      "Document: @adam_butler  Hey. It's a date! You should def come along. It's a good night. Follow @SMCSYD for updates (if you're not already). \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.2944256067276001\n",
      "Document: @cazduck Sorry. It's very odd, I know. Did you get anything? Still feeling very blah &amp; in pain when I move my head or arm too. Not ideal. \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.2657836675643921\n",
      "Document: @cazduck Oh - City to Surf is on 9 Aug... so won't be here... \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.2564896047115326\n",
      "Document: Sad to get a text on my newly returned phone telling me that Shores Restaurant, our fab wedding venue, is closing down!!  Love that place.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Processing Input Version 6: The thought occurred to me by using @MissPressa, but she did not attend SMCSYD. I caught sight of her story on Twitter.\n",
      "==================================================\n",
      "Original Score: 0.34730178117752075\n",
      "Document: @cazduck Me too. You can refresh your memory of it here if you want to:   \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.28047311305999756\n",
      "Document: @adam_butler  Hey. It's a date! You should def come along. It's a good night. Follow @SMCSYD for updates (if you're not already). \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.26602351665496826\n",
      "Document: @tiphereth Ta. Must be new profile pic day.  Thailand top of holiday list so far. Not for a few months though - probably a winter escape.\n",
      "\n",
      "==================================================\n",
      "Original Score: 0.25296202301979065\n",
      "Document: @cazduck Oh - City to Surf is on 9 Aug... so won't be here... \n",
      "\n",
      "==================================================\n",
      "Original Score: 0.25020676851272583\n",
      "Document: @cazduck Sorry. It's very odd, I know. Did you get anything? Still feeling very blah &amp; in pain when I move my head or arm too. Not ideal. \n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Process each rephrased version and the original version\n",
    "for i, input_version in enumerate([input_sentence] + rephrased_versions, start=1):\n",
    "    # Print the input version\n",
    "    print(f\"\\nProcessing Input Version {i}: {input_version}\\n{'='*50}\")\n",
    "    \n",
    "    # Retrieve relevant documents for the current version\n",
    "    input_documents = []\n",
    "    input_scores = []\n",
    "\n",
    "    # Encode the current query version\n",
    "    input_emb = model2.encode(input_version)\n",
    "\n",
    "    # Compute similarity scores between the current query version and all documents\n",
    "    similarity_scores = util.dot_score(input_emb, doc_emb)[0].cpu().tolist()\n",
    "\n",
    "    doc_score_title_pairs = list(zip([doc['text'] for doc in dataset[5]['profile']],\n",
    "                                     similarity_scores))\n",
    "\n",
    "    # Sort by decreasing similarity score\n",
    "    doc_score_title_pairs = sorted(doc_score_title_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Calculate the number of documents to retrieve (top 25%)\n",
    "    num_documents_to_retrieve = int(0.25 * len(doc_score_title_pairs))\n",
    "\n",
    "    # Check if the number of documents to retrieve is greater than 15\n",
    "    if num_documents_to_retrieve > 10:\n",
    "        num_documents_to_retrieve = 10\n",
    "\n",
    "    if num_documents_to_retrieve < 5:\n",
    "        num_documents_to_retrieve = 5\n",
    "\n",
    "    for text, score in doc_score_title_pairs[:num_documents_to_retrieve]:\n",
    "        print(f\"Original Score: {score}\")\n",
    "        print(f\"Document: {text}\\n\")\n",
    "        \n",
    "        # Store relevant document, title, and score for each interpretation\n",
    "        input_documents.append({'text': text})\n",
    "        input_scores.append(score)\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        \n",
    "    # Store relevant documents and scores for each version\n",
    "    all_relevant_documents.append(input_documents)\n",
    "    all_relevant_scores.append(input_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "75f7aced-ca1b-4d4d-8cbd-c7bfba858292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top  Documents After Average:\n",
      "[{'text': \"@adam_butler  Hey. It's a date! You should def come along. It's a good night. Follow @SMCSYD for updates (if you're not already). \"}, {'text': '@cazduck Me too. You can refresh your memory of it here if you want to:   '}, {'text': \"@cazduck Sorry. It's very odd, I know. Did you get anything? Still feeling very blah &amp; in pain when I move my head or arm too. Not ideal. \"}, {'text': \"@cazduck Oh - City to Surf is on 9 Aug... so won't be here... \"}, {'text': '@jackfaulkner Need the money  Anyway, I should have rang before 12 for sickwork, but I was sleeping off the ill, innit. Nevermind.'}, {'text': '@tiphereth Ta. Must be new profile pic day.  Thailand top of holiday list so far. Not for a few months though - probably a winter escape.'}, {'text': 'Sad to get a text on my newly returned phone telling me that Shores Restaurant, our fab wedding venue, is closing down!!  Love that place.'}]\n"
     ]
    }
   ],
   "source": [
    "# Calculate average score for each document across different interpretations\n",
    "average_scores = {}\n",
    "for documents, scores in zip(all_relevant_documents, all_relevant_scores):\n",
    "    for doc_dict, score in zip(documents, scores):\n",
    "        if isinstance(doc_dict, dict):  # Check if it's a dictionary\n",
    "            doc_text = doc_dict.get('text', '')  # Use 'get' to provide a default value if 'text' is not present\n",
    "            if doc_text:\n",
    "                if doc_text not in average_scores:\n",
    "                    average_scores[doc_text] = {'text': doc_text, 'scores': []}\n",
    "                average_scores[doc_text]['scores'].append(score)\n",
    "\n",
    "# Calculate average score for each document\n",
    "average_documents = [{'text': details['text'], 'average_score': sum(details['scores']) / len(details['scores'])}for details in average_scores.values()]\n",
    "\n",
    "# Sort documents based on average scores\n",
    "sorted_documents = sorted(average_documents, key=lambda x: x['average_score'], reverse=True)\n",
    "\n",
    "retrieval_docs = []\n",
    "\n",
    "# Output top 10 documents based on average scores\n",
    "print(\"\\nTop  Documents After Average:\")\n",
    "for doc_dict in sorted_documents[:7]:\n",
    "    new_doc_dict = {'text': doc_dict['text']}\n",
    "    retrieval_docs.append(new_doc_dict)\n",
    "print(retrieval_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ae4e3637-42b9-4c7e-bee6-309b7314767c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The text is: @adam_butler  Hey. It's a date! You should def come along. It's a good night. Follow @SMCSYD for updates (if you're not already). \n",
      "2. The text is: @cazduck Me too. You can refresh your memory of it here if you want to:   \n",
      "3. The text is: @cazduck Sorry. It's very odd, I know. Did you get anything? Still feeling very blah &amp; in pain when I move my head or arm too. Not ideal. \n",
      "4. The text is: @cazduck Oh - City to Surf is on 9 Aug... so won't be here... \n",
      "5. The text is: @jackfaulkner Need the money  Anyway, I should have rang before 12 for sickwork, but I was sleeping off the ill, innit. Nevermind.\n",
      "6. The text is: @tiphereth Ta. Must be new profile pic day.  Thailand top of holiday list so far. Not for a few months though - probably a winter escape.\n",
      "7. The text is: Sad to get a text on my newly returned phone telling me that Shores Restaurant, our fab wedding venue, is closing down!!  Love that place.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r_d = ''\n",
    "count = 1\n",
    "for i in retrieval_docs:\n",
    "  # print(i)\n",
    "  r_d += str(count) + '. '\n",
    "  r_d += 'The text is: ' + i['text'] + '\\n'\n",
    "  count += 1\n",
    "  # if count > 7:\n",
    "  #   break\n",
    "\n",
    "print(r_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "666332bf-51c6-475c-bcf6-98101d82a392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U -q google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b75c4119-762f-48f4-9841-3cc428717a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as palm\n",
    "\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "760ba00d-ccb2-4495-9465-a1e724b5e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "palm.configure(api_key='AIzaSyBVo_JbfzrPBpHbueQtOiRozzyFK1QK8D0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b5c071a0-80b9-4bc9-915f-09e9e052d1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'query' is defined with a meaningful value\n",
    "query = \"@MissPressa and I had the same idea. Unfortunately, she did not attend SMCSYD. I noticed her experience via Twitter.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b9350c28-8a98-4cf5-88f0-3befc901c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(query, relevant_passage):\n",
    "  escaped = relevant_passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
    "  prompt = textwrap.dedent(\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below.   \n",
    "  Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. I'm providing you with some sample tweets written by me for 7 texts examples.\n",
    "  Based on the context provided, can you paraphrase the query to capture my writing style. Make sure it looks like its written by me.\n",
    "  QUESTION: '{query}'\n",
    "  PASSAGE: '{relevant_passage}'\n",
    "\n",
    "    ANSWER:\n",
    "  \"\"\").format(query=query, relevant_passage=escaped)\n",
    "\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0b0e2050-18a0-41c7-984e-368a7404b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "passage = r_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f612289e-8632-435b-b591-021cc5523a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful and informative bot that answers questions using text from the reference passage included below.   \n",
      "  Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. I'm providing you with some sample tweets written by me for 7 texts examples.\n",
      "  Based on the context provided, can you paraphrase the query to capture my writing style. Make sure it looks like its written by me.\n",
      "  QUESTION: '@MissPressa and I had the same idea. Unfortunately, she did not attend SMCSYD. I noticed her experience via Twitter.'\n",
      "  PASSAGE: '1. The text is: @adam_butler  Hey. Its a date! You should def come along. Its a good night. Follow @SMCSYD for updates (if youre not already).  2. The text is: @cazduck Me too. You can refresh your memory of it here if you want to:    3. The text is: @cazduck Sorry. Its very odd, I know. Did you get anything? Still feeling very blah &amp; in pain when I move my head or arm too. Not ideal.  4. The text is: @cazduck Oh - City to Surf is on 9 Aug... so wont be here...  5. The text is: @jackfaulkner Need the money  Anyway, I should have rang before 12 for sickwork, but I was sleeping off the ill, innit. Nevermind. 6. The text is: @tiphereth Ta. Must be new profile pic day.  Thailand top of holiday list so far. Not for a few months though - probably a winter escape. 7. The text is: Sad to get a text on my newly returned phone telling me that Shores Restaurant, our fab wedding venue, is closing down!!  Love that place. '\n",
      "\n",
      "    ANSWER:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = make_prompt(query, passage)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cea07f89-c367-4cde-bf19-e0dc2d529c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "\n",
    "text_model = text_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "67d98bdc-4894-4c48-aa4c-64e1e7e3ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.6\n",
    "answer = palm.generate_text(prompt=prompt,\n",
    "                            model=text_model,\n",
    "                            candidate_count=3,\n",
    "                            temperature=temperature,\n",
    "                            max_output_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "887f35f7-0026-4437-b968-b68534cb50b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 0: @MissPressa and I had the same idea. Unfortunately, she was unable to attend SMCSYD. I was informed of her experience via Twitter.\n",
      "\n",
      "Candidate 1: @MissPressa and I had the same idea, however she didn't attend SMCSYD, unfortunately. I noticed her experience via Twitter.\n",
      "\n",
      "Candidate 2: MissPressa and I shared the same idea of attending SMCSYD, unfortunately she was unable to make it, but I noticed her experience through Twitter.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, candidate in enumerate(answer.candidates):\n",
    "  print(f\"Candidate {i}: {candidate['output']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1a117-e5a0-45c9-a1e0-a57e4e690699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
